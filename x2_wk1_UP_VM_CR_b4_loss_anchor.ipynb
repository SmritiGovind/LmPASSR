{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22510,"status":"ok","timestamp":1716458954706,"user":{"displayName":"SmG Savitri","userId":"12776258865114306376"},"user_tz":-330},"id":"zG7e-p4IjE9U","outputId":"5f5ec701-64a9-41a3-f023-2a1898e5ef8b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5639,"status":"ok","timestamp":1716459041782,"user":{"displayName":"SmG Savitri","userId":"12776258865114306376"},"user_tz":-330},"id":"fNv458k0e3RI","outputId":"66efe9cc-01d4-48a8-f205-4a6cadd4347f"},"outputs":[{"output_type":"stream","name":"stdout","text":["len(train_dataloader): 112\n"]}],"source":["# dataset\n","\n","import cv2\n","import os\n","import torch\n","import random\n","import numpy as np\n","from torch.utils.data import Dataset\n","from torch.utils.data import DataLoader\n","\n","\n","def create_dataloader(split, SR_rate, augment, batch_size=1, shuffle=False, num_workers=1, pin_memory=True):\n","    dataset = dataread(split, SR_rate, augment)\n","    dataloader = DataLoader(dataset, batch_size, shuffle=shuffle, num_workers=num_workers, pin_memory=pin_memory)\n","    # print('check 0', dataloader) #me\n","    return dataloader\n","\n","def random_crop(LR_img, HR_img, crop_size, SR_rate):\n","    # check the shape\n","    # print('LR_img.shape', LR_img.shape) #me\n","    # print('HR_img.shape', HR_img.shape) #me\n","    LR_h, LR_w = LR_img.shape[:2]\n","    HR_h, HR_w = HR_img.shape[:2]\n","    # print('np.round_(LR_h * SR_rate)=', np.round_(LR_h * SR_rate), ', HR_h=', HR_h)\n","    # assert np.round_(LR_h * SR_rate) == HR_h and np.round_(LR_w * SR_rate) == HR_w, 'SR_rate is not correct for LR and HR image'\n","    # check the crop size\n","    new_LR_h, new_LR_w = crop_size\n","    assert new_LR_h <= LR_h and new_LR_w <= LR_w, 'crop_size is too large'\n","\n","    y1 = random.randint(0, LR_h - new_LR_h)\n","    x1 = random.randint(0, LR_w - new_LR_w)\n","\n","    LR_crop = LR_img[y1:y1 + new_LR_h, x1:x1 + new_LR_w, :]\n","    HR_crop = HR_img[SR_rate * y1:SR_rate * (y1 + new_LR_h), SR_rate * x1:SR_rate * (x1 + new_LR_w), :]\n","\n","    return LR_crop, HR_crop\n","\n","class dataread(Dataset):  # for training/testing\n","    def __init__(self, split, SR_rate, augment=False):\n","\n","        self.split = split\n","        self.SR_rate = SR_rate\n","        self.augment = augment\n","        self.intensity_list = [1.0, 0.7, 0.5]\n","        self.crop_size = [32, 32]\n","\n","        # data split\n","        if split == 'train':\n","            #self.LR_dir = os.path.join('/content/drive/MyDrive/ColabNotebooks/Research_works/dataset/DIV2K/DIV2K_train_LR_bicubic/', 'X'+str(SR_rate))\n","            self.LR_dir_l = os.path.join('/content/drive/MyDrive/phd/StereoSR/datasets/Flickr_modifd/LR/X2/separated/Train/left')\n","            self.LR_dir_r = os.path.join('/content/drive/MyDrive/phd/StereoSR/datasets/Flickr_modifd/LR/X2/separated/Train/right')\n","            self.HR_dir_l = '/content/drive/MyDrive/phd/StereoSR/datasets/Flickr_modifd/HR/size_corrected/hr_x2/train/left'\n","            self.HR_dir_r = '/content/drive/MyDrive/phd/StereoSR/datasets/Flickr_modifd/HR/size_corrected/hr_x2/train/right'\n","            self.img_names_l = sorted(os.listdir(self.LR_dir_l))\n","            self.img_names_r = sorted(os.listdir(self.LR_dir_r))\n","            # print('Sorted HR_l images upto 5: ', self.img_names_l)\n","            # print('Sorted HR_r images upto 5: ', self.img_names_r)\n","            # print('LR_dir_l: ', self.LR_dir_l)\n","            # print('LR_dir_r: ', self.LR_dir_r)\n","            # print('HR_dir_l: ',self.HR_dir_l)\n","            # print('HR_dir_r: ',self.HR_dir_r)\n","\n","        elif split == 'valid':\n","            #self.LR_dir = os.path.join('/content/drive/MyDrive/ColabNotebooks/Research_works/dataset/DIV2K/DIV2K_train_LR_bicubic/', 'X'+str(SR_rate))\n","            self.LR_dir_l = os.path.join('/content/drive/MyDrive/phd/StereoSR/datasets/Flickr_modifd/LR/X2/separated/val/left')\n","            self.LR_dir_r = os.path.join('/content/drive/MyDrive/phd/StereoSR/datasets/Flickr_modifd/LR/X2/separated/val/right')\n","            self.HR_dir_l = '/content/drive/MyDrive/phd/StereoSR/datasets/Flickr_modifd/HR/size_corrected/hr_x2/val/left'\n","            self.HR_dir_r = '/content/drive/MyDrive/phd/StereoSR/datasets/Flickr_modifd/HR/size_corrected/hr_x2/val/right'\n","            self.img_names_l = sorted(os.listdir(self.HR_dir_l))\n","            self.img_names_r = sorted(os.listdir(self.HR_dir_r))\n","        elif split == 'test':\n","\n","            # self.LR_dir_l = os.path.join('/content/drive/MyDrive/phd/StereoSR/datasets/kitti_mix/LR/x2/left')\n","            # self.LR_dir_r = os.path.join('/content/drive/MyDrive/phd/StereoSR/datasets/kitti_mix/LR/x2/right')\n","            # self.HR_dir_l = '/content/drive/MyDrive/phd/StereoSR/datasets/kitti_mix/HR_size_crctd/x2/left'\n","            # self.HR_dir_r = '/content/drive/MyDrive/phd/StereoSR/datasets/kitti_mix/HR_size_crctd/x2/right'\n","\n","            self.LR_dir_l = os.path.join('/content/drive/MyDrive/phd/StereoSR/datasets/x2_versions/x2_flickr/LR/left')\n","            self.LR_dir_r = os.path.join('/content/drive/MyDrive/phd/StereoSR/datasets/x2_versions/x2_flickr/LR/right')\n","            self.HR_dir_l = '/content/drive/MyDrive/phd/StereoSR/datasets/x2_versions/x2_flickr/HR_size_crctd/left'\n","            self.HR_dir_r = '/content/drive/MyDrive/phd/StereoSR/datasets/x2_versions/x2_flickr/HR_size_crctd/right'\n","\n","            # self.LR_dir_l = os.path.join('/content/drive/MyDrive/phd/StereoSR/datasets/kitti_mix/LR/x2/left')\n","            # self.LR_dir_r = os.path.join('/content/drive/MyDrive/phd/StereoSR/datasets/kitti_mix/LR/x2/right')\n","            # self.HR_dir_l = '/content/drive/MyDrive/phd/StereoSR/datasets/kitti_mix/HR_size_crctd/x2/left'\n","            # self.HR_dir_r = '/content/drive/MyDrive/phd/StereoSR/datasets/kitti_mix/HR_size_crctd/x2/right'\n","\n","            self.img_names_l = sorted(os.listdir(self.HR_dir_l))\n","            self.img_names_r = sorted(os.listdir(self.HR_dir_r))\n","        else:\n","            raise NameError('data split must be \"train\", \"valid\" or \"test\". ')\n","\n","\n","\n","    def __len__(self):\n","\n","\n","        if self.split == 'train':\n","           return len(self.img_names_l)\n","        else:\n","          return len(self.img_names_r)\n","\n","\n","\n","    def __getitem__(self, index):\n","\n","        if self.split == 'train':\n","            #LR_img = cv2.imread(os.path.join(self.LR_dir, self.img_names[index][:-4]+'x'+str(self.SR_rate)+'.png')) / 255.  #me\n","\n","            #load  LR stereo\n","            LR_path_l = os.path.join(self.LR_dir_l, self.img_names_l[index])\n","            LR_path_r = os.path.join(self.LR_dir_r, self.img_names_r[index])\n","            # print('LR_path_l: ',LR_path_l)\n","            # print('LR_path_r: ',LR_path_r)\n","            # print('type_l: ', type(LR_path_l))\n","            # print('type_r: ', type(LR_path_r))\n","            LR_img1_l = cv2.imread(LR_path_l)  #me\n","            LR_img1_r = cv2.imread(LR_path_r)  #me\n","            # print(type(LR_img1_l))\n","            LR_img_l = LR_img1_l/255\n","            LR_img_r = LR_img1_r/255\n","\n","            #load HR stereo\n","            HR_img_l = cv2.imread(os.path.join(self.HR_dir_l, self.img_names_l[index])) / 255.\n","            HR_img_r = cv2.imread(os.path.join(self.HR_dir_r, self.img_names_r[index])) / 255\n","\n","            if self.augment:\n","                # random crop\n","                LR_img_l, HR_img_l = random_crop(LR_img_l, HR_img_l, self.crop_size, self.SR_rate)\n","                LR_img_r, HR_img_r = random_crop(LR_img_r, HR_img_r, self.crop_size, self.SR_rate)\n","\n","                # geometric transformations\n","                if random.random() < 0.5: # hflip\n","                    LR_img_l, LR_img_r = LR_img_l[:, ::-1, :], LR_img_r[:, ::-1, :]\n","                    HR_img_l, HR_img_r = HR_img_l[:, ::-1, :], HR_img_r[:, ::-1, :]\n","                if random.random() < 0.5: # vflip\n","                    LR_img_l, LR_img_r = LR_img_l[::-1, :, :], LR_img_r[::-1, :, :]\n","                    HR_img_l, HR_img_r = HR_img_l[::-1, :, :], HR_img_r[::-1, :, :]\n","                if random.random() < 0.5: # rot90\n","                    LR_img_l, LR_img_r = LR_img_l.transpose(1, 0, 2), LR_img_r.transpose(1, 0, 2)\n","                    HR_img_l, HR_img_r = HR_img_l.transpose(1, 0, 2), HR_img_r.transpose(1, 0, 2)\n","\n","                # intensity scale\n","                intensity_scale = random.choice(self.intensity_list)\n","                LR_img_l *= intensity_scale\n","                LR_img_r *= intensity_scale\n","                HR_img_l *= intensity_scale\n","                HR_img_r *= intensity_scale\n","\n","\n","        else:\n","            #LR_img = cv2.imread(os.path.join(self.LR_dir, self.img_names[index][:-4]+'x'+str(self.SR_rate)+'.png')) / 255. #me\n","            LR_img_l = cv2.imread(os.path.join(self.LR_dir_l, self.img_names_l[index])) / 255.  #me\n","            LR_img_r = cv2.imread(os.path.join(self.LR_dir_r, self.img_names_r[index])) / 255.\n","            HR_img_l = cv2.imread(os.path.join(self.HR_dir_l, self.img_names_l[index])) / 255.\n","            HR_img_r = cv2.imread(os.path.join(self.HR_dir_r, self.img_names_r[index])) / 255.\n","\n","        # Convert\n","        LR_img_l = np.ascontiguousarray(LR_img_l.transpose(2, 0, 1)) # HWC => CHW\n","        LR_img_r = np.ascontiguousarray(LR_img_r.transpose(2, 0, 1))\n","        HR_img_l = np.ascontiguousarray(HR_img_l.transpose(2, 0, 1))\n","        HR_img_r = np.ascontiguousarray(HR_img_r.transpose(2, 0, 1))\n","\n","        #return torch.from_numpy(LR_img_l), torch.from_numpy(LR_img_r), torch.from_numpy(HR_img_l), torch.from_numpy(HR_img_r), self.img_names_l[index],self.img_names_r[index]\n","        return torch.from_numpy(LR_img_l), torch.from_numpy(LR_img_r), torch.from_numpy(HR_img_l), torch.from_numpy(HR_img_r), self.img_names_l[index],self.img_names_r[index]\n","\n","if __name__ == '__main__':\n","    os.makedirs('/content/drive/MyDrive/phd/wk1/phase1_baseline/Output_UP_VM_CR_b4_loss_anchor/x2/test_dataloader', exist_ok=True)\n","    train_dataloader = create_dataloader('test',2, False, batch_size=1, shuffle=False, num_workers=1)\n","    print(f\"len(train_dataloader): {len(train_dataloader)}\")\n","    #LR_img, HR_img, img_names = next(iter(train_dataloader))\n","    # iterator = iter(train_dataloader) #me\n","    # LR_img_l, LR_img_r, HR_img_l, HR_img_r, img_names_l, img_names_r = next(iterator)\n","    # # #LR_img = next(iterator)\n","    # # print(f\"LR_img shape: {LR_img_l.size()}\")\n","    # # print(f\"LR_img shape: {LR_img_r.size()}\")\n","    # # print(f\"HR_img shape: {HR_img_l.size()}\")\n","    # # print(f\"HR_img shape: {HR_img_r.size()}\")\n","    # # print('left image names in iterator: ', img_names_l, len(img_names_l))\n","    # # print('right image names in iterator: ', img_names_r, len(img_names_r))\n","    # LR_img_l = LR_img_l[0].numpy().transpose(1, 2, 0)\n","    # LR_img_r = LR_img_r[0].numpy().transpose(1, 2, 0)\n","    # HR_img_l = HR_img_l[0].numpy().transpose(1, 2, 0)\n","    # HR_img_r = HR_img_r[0].numpy().transpose(1, 2, 0)\n","    # cv2.imwrite('./test_dataloader/LR_img_l.png', np.uint8(LR_img_l*255))\n","    # cv2.imwrite('./test_dataloader/LR_img_r.png', np.uint8(LR_img_r*255))\n","    # cv2.imwrite('./test_dataloader/HR_img_l.png', np.uint8(HR_img_l*255))\n","    # cv2.imwrite('./test_dataloader/HR_img_r.png', np.uint8(HR_img_r*255))"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":411,"status":"ok","timestamp":1716459074458,"user":{"displayName":"SmG Savitri","userId":"12776258865114306376"},"user_tz":-330},"id":"3PxE29Cl1LFi"},"outputs":[],"source":["# visualization\n","\n","import os\n","import cv2\n","\n","import torch\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","def save_res(pred_HR_l, pred_HR_r, img_names_l, img_names_r, folder):\n","    '''\n","\n","    Parameters\n","    ----------\n","    preds : List\n","        each pred has a shape of 1x3xHxW. BGR\n","    img_names : List\n","\n","    Returns\n","    -------\n","    None.\n","\n","    '''\n","    # for pred_HR_l, pred_HR_r, img_names_l, img_names_r in zip(pred_HR_l, pred_HR_r, img_names_l, img_names_r):\n","    #     pred_img_l = pred[0].cpu().numpy().transpose(1,2,0)\n","    #     pred_img_r = pred[0].cpu().numpy().transpose(1,2,0)\n","    #     cv2.imwrite(os.path.join(save_dir, img_names_l), np.uint8(pred_img_l*255))\n","    #     cv2.imwrite(os.path.join(save_dir, img_names_r), np.uint8(pred_img_r*255))\n","    # return\n","\n","    for pred_HR_l, pred_HR_r, img_name_l, img_name_r in zip(pred_HR_l, pred_HR_r, img_names_l, img_names_r):\n","        pred_img_l = pred_HR_l[0].detach().cpu().numpy().transpose(1, 2, 0)\n","        pred_img_r = pred_HR_r[0].detach().cpu().numpy().transpose(1, 2, 0)\n","        cv2.imwrite(os.path.join(folder, img_name_l), np.uint8(pred_img_l * 255))\n","        cv2.imwrite(os.path.join(folder, img_name_r), np.uint8(pred_img_r * 255))\n","    return\n","\n","\n","def visualize_training(save_dir,epoch_start):\n","    txt_res = os.path.join(save_dir, f\"result_epoch_{epoch_start}.txt\")\n","    with open(txt_res, 'r') as f:\n","        info = f.readlines()\n","    epoch, lr, train_loss, valid_loss, psnr = [], [], [], [], []\n","\n","    for line in info[:-1]:\n","        line = line.strip().split('|')\n","        epoch.append(int(line[0].split(':')[1]))\n","        lr.append(float(line[1].split(':')[1]))\n","        train_loss.append(float(line[2].split(':')[1]))\n","        valid_loss.append(float(line[3].split(':')[1]))\n","        psnr.append(float(line[4].split(':')[1]))\n","\n","    fig = plt.figure(figsize=(16, 8), dpi=400)\n","    ax1 = fig.add_subplot(221)\n","    ax2 = fig.add_subplot(222)\n","    ax3 = fig.add_subplot(223)\n","    ax4 = fig.add_subplot(224)\n","    ax1.title.set_text('Training loss')\n","    ax2.title.set_text('Validation loss')\n","    ax3.title.set_text('PSNR (validation)')\n","    ax4.title.set_text('learning rate')\n","    ax1.plot(epoch, train_loss)\n","    ax2.plot(epoch, valid_loss)\n","    ax3.plot(epoch, psnr)\n","    ax4.plot(epoch, lr)\n","    plt.savefig(os.path.join(save_dir, 'results.png'), dpi=200)\n","    return"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1716459076918,"user":{"displayName":"SmG Savitri","userId":"12776258865114306376"},"user_tz":-330},"id":"Zo9WNPf1igf5"},"outputs":[],"source":["# metric\n","\n","\n","import torch\n","\n","def cal_psnr(x, y):\n","    '''\n","    Parameters\n","    ----------\n","    x, y are two tensors has the same shape (1, C, H, W)\n","\n","    Returns\n","    -------\n","    score : PSNR.\n","    '''\n","\n","    mse = torch.mean((x - y) ** 2, dim=[1, 2, 3])\n","    score = - 10 * torch.log10(mse)\n","    return score"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1716459078139,"user":{"displayName":"SmG Savitri","userId":"12776258865114306376"},"user_tz":-330},"id":"aRrHaTqziuKB"},"outputs":[],"source":["# loss\n","\n","import torch\n","from torch import nn\n","\n","class L1Loss(object):\n","    def __call__(self, input, target):\n","        return torch.abs(input - target).mean()\n","\n","class CharbonnierLoss(nn.Module):\n","\n","    def __init__(self, eps=0.01):\n","        super(CharbonnierLoss, self).__init__()\n","        self.eps = eps\n","\n","    def forward(self, pred, gt):\n","        # print(len(pred),'&', len(gt))\n","        loss = torch.sqrt((pred - gt)**2 + self.eps).mean()\n","\n","        return loss.mean()"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1213,"status":"ok","timestamp":1716459081170,"user":{"displayName":"SmG Savitri","userId":"12776258865114306376"},"user_tz":-330},"id":"_sGJZDxlxcHt","outputId":"c20de607-48a2-46ec-c228-4703eba6a16e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Total parameters = 776551\n"]}],"source":["########################## new model #############################\n","import torch\n","import torch.nn as nn\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import math\n","from torch.quantization import QuantStub, DeQuantStub\n","from torchsummary import summary\n","from skimage import morphology\n","import torch.nn.functional as F\n","\n","class ClippedReLU(nn.Module):\n","    def __init__(self):\n","        super(ClippedReLU, self).__init__()\n","\n","    def forward(self, x):\n","        return x.clamp(min=0., max=1.)\n","\n","class ConvRelu(nn.Module):\n","    def __init__(self, in_channels, out_channels, kernel_size, bias=True):\n","        super(ConvRelu, self).__init__()\n","        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, padding=(kernel_size//2), bias=bias)\n","        self.relu = nn.ReLU(inplace=True)\n","\n","    def forward(self, x):\n","        # print(\"input tensor shape @ clipped relu =\", x.shape)\n","        x = self.conv(x)\n","        x = self.relu(x)\n","        return x\n","\n","class Dblock(nn.Module):\n","    def __init__(self, in_channels, out_channels, groups):\n","        super(Dblock, self).__init__()\n","        self.conv0 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, groups=groups)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.conv1 = nn.Conv2d(out_channels, out_channels, kernel_size=1, padding=0)\n","\n","    def forward(self, x):\n","        x = self.conv0(x)\n","        x = self.relu(x)\n","        x = self.conv1(x)\n","        return x\n","\n","class ResB(nn.Module):\n","    def __init__(self, channels):\n","        super(ResB, self).__init__()\n","        self.body = nn.Sequential(\n","            nn.Conv2d(channels, channels, 3, 1, 1, bias=False),\n","            nn.LeakyReLU(0.1, inplace=True),\n","            nn.Conv2d(channels, channels, 3, 1, 1, bias=False),\n","        )\n","    def __call__(self,x):\n","        out = self.body(x)\n","        return out + x\n","\n","class SaveValues:\n","    def __init__(self):\n","        self.weights = None\n","        self.activations = None\n","        self.device = 'cuda'\n","        # self.device = 'cpu'\n","    def hook_weights(self, module, input, output):\n","        self.weights = module.weight.data.to(self.device).view(-1)\n","    def hook_activations(self, module, input, output):\n","        self.activations = output.data.to(self.device)\n","    # def hook_fn_grad(self, module, grad_input, grad_output):\n","    #     self.gradients = grad_output[0]\n","    def clear(self):\n","        self.weights = None\n","        self.activations = None\n","\n","class Linear_Activation(nn.Module):\n","    def __init__(self):\n","        super(Linear_Activation, self).__init__()\n","\n","        self.linear = None  # Define linear layer here\n","        self.save_values = SaveValues()\n","\n","    def forward(self, x):\n","\n","        # Subtract mean along the last dimension\n","        device = 'cuda'\n","        # device = 'cpu'\n","        channels, height, width = x.size()\n","        # print('channels, height, width of i/p linear activation=', x.size)\n","        x_flatten = x\n","        x_mean = torch.mean(x_flatten, dim=-1, keepdim=True)\n","        x_subtracted = x_flatten - x_mean\n","        # print('x_subtracted', x_subtracted.shape)\n","\n","        x_subtracted_cpu = x_subtracted.to(device)\n","\n","        # Create the linear layer dynamically based on the input width\n","        self.linear = nn.Linear(width, height)\n","        # print(self.linear.weight.shape)\n","        self.linear.to(device)\n","\n","         # Register hooks for the b1 layers\n","        hook_handle_weights = self.linear.register_forward_hook(self.save_values.hook_weights)\n","\n","        # Apply linear transformation\n","        x_transformed = self.linear(x_subtracted_cpu)\n","\n","        # Access the saved weights and activations\n","        weights_linear = self.save_values.weights\n","        # activations_linear = self.save_values.activations\n","        # Remove the hooks\n","        hook_handle_weights.remove()\n","        # hook_handle_activations.remove()\n","\n","        # print('weights_linear o/p linear activation=', weights_linear.size)\n","\n","        # return weights_linear, activations_linear\n","        return weights_linear\n","\n","class Get_weights_activations(nn.Module):\n","    def __init__(self, input_dim):\n","        super(Get_weights_activations, self).__init__()\n","        self.conv2 = None\n","        self.save_values = SaveValues()\n","\n","    def forward(self, x):\n","\n","        device = 'cuda'\n","        # device = 'cpu'\n","        # Assuming x is your input tensor on the GPU\n","        batch_size, channels, height, width = x.size()\n","        # print('input size of getActivation = ',batch_size, channels, height, width )\n","        x_cpu = x.to(device)\n","        self.conv2 = nn.Conv2d(in_channels=64, out_channels=1, kernel_size=3, padding=1)\n","        self.conv2.to(device)\n","        hook_handle_activations = self.conv2.register_forward_hook(self.save_values.hook_activations)\n","        # convolutional layer\n","        x_out = self.conv2(x_cpu)\n","        # Access the saved weights and activations\n","        # weights_x_out = self.save_values.weights\n","        activations_x_out = self.save_values.activations\n","        # print('activations_x_out = ', activations_x_out.shape)\n","        # Remove the hooks\n","        # hook_handle_weights.remove()\n","        hook_handle_activations.remove()\n","\n","        # return weights_x_out, activations_x_out\n","        return activations_x_out\n","\n","class Valid_Mask_CAM(object):\n","    \"\"\" Class Activation Mapping \"\"\"\n","\n","    def __init__(self, channels):\n","\n","        self.linear_activation = Linear_Activation()\n","        self.get_attention = Get_weights_activations(channels)\n","\n","    def forward(self, A, B):\n","\n","        b2,c2,h2,w2 = A.shape\n","        B = B.permute(1,0,2)\n","        # print('Input to linear actvtn = ', B.shape)\n","        values_A = self.get_attention(A)\n","        values_B = self.linear_activation(B)\n","\n","        # b, c, h, w = values_A[1].shape\n","        b, c, h, w = values_A.shape\n","        linear_layer_output = values_B\n","        # print('output of linear actvtn weights.shape = ', linear_layer_output.shape)\n","        linear_layer_output = linear_layer_output.unsqueeze(0).unsqueeze(0)\n","        linear_layer_output = linear_layer_output.view(b,c,h,w)\n","        # print('output of linear actvtn weights.shape = ', linear_layer_output.shape)\n","\n","        valid_mask_cam = torch.mul(values_A, linear_layer_output)\n","\n","        # Set values greater than 0.1 to 0\n","        valid_mask_cam[valid_mask_cam > 0.1] = 0\n","\n","        # Set negative values to 0\n","        valid_mask_cam[valid_mask_cam < 0] = 0\n","\n","        return valid_mask_cam.data\n","\n","    def __call__(self, A, B):\n","        return self.forward(A, B)\n","\n","class PAM(nn.Module):\n","    def __init__(self, channels):\n","        super(PAM, self).__init__()\n","        self.b1 = nn.Conv2d(channels, channels, 1, 1, 0, bias=True)\n","        self.b2 = nn.Conv2d(channels, channels, 1, 1, 0, bias=True)\n","        self.b3 = nn.Conv2d(channels, channels, 1, 1, 0, bias=True)\n","        self.b4 = nn.Conv2d(channels, channels, 1, 1, 0, bias=True)\n","        self.softmax = nn.Softmax(-1)\n","        self.rb = ResB(64)\n","        self.fusion = nn.Conv2d(channels * 2 + 1, channels, 1, 1, 0, bias=True)\n","        self.valid_mask = Valid_Mask_CAM(channels)\n","\n","    def __call__(self, x_left, x_right, is_training):\n","        b, c, h, w = x_left.shape\n","        buffer_left = self.rb(x_left)\n","        buffer_right = self.rb(x_right)\n","\n","        ### M_{right_to_left}\n","        Q = self.b1(buffer_left).permute(0, 2, 3, 1)                                                # B * H * W * C\n","        S = self.b2(buffer_right).permute(0, 2, 1, 3)                                               # B * H * C * W\n","        # print(\"Q shape:\", Q.shape)\n","        # print(\"S shape:\", S.shape)\n","        score_l = torch.bmm(Q.contiguous().view(-1, w, c),\n","                          S.contiguous().view(-1, c, w))                                            # (B*H) * W * W\n","        M_right_to_left = self.softmax(score_l)\n","\n","        ### M_{left_to_right}\n","        Q = self.b1(buffer_right).permute(0, 2, 3, 1)                                               # B * H * W * C\n","        S = self.b2(buffer_left).permute(0, 2, 1, 3)                                                # B * H * C * W\n","        '''the input tensors Q and S may represent the hidden states of two\n","        different encoders. '''\n","        score_r = torch.bmm(Q.contiguous().view(-1, w, c),\n","                          S.contiguous().view(-1, c, w))                                            # (B*H) * W * W\n","        # '''what Score do? The torch.bmm() operation can be used to calculate the\n","        # similarity between the two hidden states (Q,S), which can be used to predict\n","        # the relationship between the two input sequences.'''\n","        M_left_to_right = self.softmax(score_r)\n","        # M_left_to_right = self.softmax(score_l.permute(0, 2, 1))\n","\n","        Vin_L = self.b3(x_left)\n","        Vin_R = self.b3(x_right)\n","        # print('Vmask_l which is A in valid_CAM =', Vin_L.shape)\n","        # print('M_right_to_left which is B in valid_CAM =', M_right_to_left.shape)\n","        V_right_left = self.valid_mask(Vin_L, M_right_to_left)\n","        V_left_right = self.valid_mask(Vin_R, M_left_to_right)\n","        # V_right_left = V_right_left.view(b, 1, h, w)\n","        # V_left_right = V_left_right.view(b, 1, h, w)\n","        # print('V_right_left =', V_right_left.shape)\n","\n","        if is_training==1:\n","          # V_right_left = self.valid_mask(Vmask_L, M_right_to_left)\n","          # V_left_right = self.valid_mask(Vmask_R, M_left_to_right)\n","          M_left_right_left = torch.bmm(M_right_to_left, M_left_to_right)\n","          M_right_left_right = torch.bmm(M_left_to_right, M_right_to_left)\n","\n","        ### fusion\n","        buffer_l = self.b4(x_right).permute(0,2,3,1).contiguous().view(-1, w, c)                      # (B*H) * W * C\n","        buffer_l = torch.bmm(M_right_to_left, buffer_l).contiguous().view(b, h, w, c).permute(0,3,1,2)  #  B * C * H * W\n","        # print('buffer_l', buffer_l.shape)\n","        # print('x_left.shape',x_left.shape)\n","        out_l = self.fusion(torch.cat((buffer_l, V_left_right, x_left), 1))\n","\n","        buffer_r = self.b4(x_left).permute(0,2,3,1).contiguous().view(-1, w, c)                      # (B*H) * W * C\n","        buffer_r = torch.bmm(M_left_to_right, buffer_r).contiguous().view(b, h, w, c).permute(0,3,1,2)  #  B * C * H * W\n","        out_r = self.fusion(torch.cat((buffer_r, V_right_left, x_right), 1))\n","        '''This Conv2d layer in 'out' is  used to fuse the original input tensor\n","         with its reflection and a channel of 1s. This helps the network learn\n","         to distinguish between foreground and background pixels in the image.'''\n","\n","        ## output\n","        if is_training == 1:\n","            return out_l, out_r, \\\n","               (M_right_to_left.contiguous().view(b, h, w, w), M_left_to_right.contiguous().view(b, h, w, w)), \\\n","               (M_left_right_left.view(b,h,w,w), M_right_left_right.view(b,h,w,w)), \\\n","               (V_left_right, V_right_left)\n","        if is_training == 0:\n","            return out_l, out_r\n","\n","\n","class XLSR_stereo_AM(nn.Module):\n","    def __init__(self, SR_rate):\n","        super(XLSR_stereo_AM, self).__init__()\n","        self.upscale_factor = SR_rate\n","\n","        ### feature extraction\n","        self.init_feature  = nn.Sequential(nn.Conv2d(in_channels=3, out_channels=8, kernel_size=3,padding=1),\n","                                            nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3, padding=1),\n","                                           nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1),\n","                                            ConvRelu(in_channels=32, out_channels=64, kernel_size=1))\n","\n","        ### paralax attention\n","        self.pam = PAM(64)\n","\n","        ### Dblocks\n","        self.Dblocks = nn.Sequential(Dblock(64, 64, 4),Dblock(64, 64, 4),\n","                                     Dblock(64, 64, 4))\n","\n","        self.conv1 = nn.Sequential(nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1),\n","                                   nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1),\n","                                   nn.LeakyReLU(0.1, inplace=True))\n","\n","\n","        ########### Anchors making\n","        self.anch_func = lambda x_list: torch.cat(x_list, dim=1)\n","        self.conv_anch = nn.Conv2d(in_channels=30, out_channels=64, kernel_size=3, padding=1)\n","\n","        #### upscale module\n","        self.conv2 = nn.Conv2d(in_channels=64, out_channels=64*SR_rate**2, kernel_size=3, padding=1)\n","        self.clippedReLU = ClippedReLU()\n","        self.upscale = nn.Sequential(nn.PixelShuffle(SR_rate),\n","            nn.Conv2d(in_channels=64, out_channels=3, kernel_size=3, padding=1),  # No need for bias=False here\n","            nn.LeakyReLU(0.1, inplace=True),  # Add LeakyReLU activation after the last convolution\n","            nn.Conv2d(in_channels=3, out_channels=3, kernel_size=3, padding=1),  # Add one more convolution with padding\n","            )\n","\n","        # weights initialization\n","        for m in self.modules():\n","            if isinstance(m, nn.Conv2d):\n","                # nn.init.kaiming_normal_(m.weight.data, mode='fan_out', nonlinearity='relu')\n","                _, fan_out = torch.nn.init._calculate_fan_in_and_fan_out(m.weight.data)\n","                std = math.sqrt(2/fan_out*0.1)\n","                torch.nn.init.normal_(m.weight.data, mean=0, std=std)\n","                if m.bias is not None:\n","                    nn.init.constant_(m.bias.data, 0.01)\n","\n","\n","    def forward(self, x_left, x_right, is_training):\n","        ### feature extraction\n","        buffer_left = self.init_feature(x_left)\n","        buffer_right = self.init_feature(x_right)\n","\n","        # print(\"x_left shape:\", x_left.shape)\n","\n","        if is_training == 1:\n","            ### parallax attention\n","            buffer_lt,buffer_rt, (M_right_to_left, M_left_to_right), (M_left_right_left, M_right_left_right), \\\n","            (V_left_to_right, V_right_to_left) = self.pam(buffer_left, buffer_right, is_training)\n","\n","            # print(\"PAM 1st PAM shape:\", buffer.shape)\n","\n","            ##### G-block\n","            res1_l = self.Dblocks(buffer_lt)\n","            res1_r = self.Dblocks(buffer_rt)\n","\n","            # print(\"o/p of Dblock shape:\", res1.shape)\n","\n","\n","            res2_l = self.conv1(x_left)\n","            res2_r = self.conv1(x_right)\n","\n","            # print(\"res2_l shape:\", res2_l.shape)\n","            # print(\"res2_r shape:\", res2_r.shape)\n","\n","            ### parallax attention_left\n","            buffer_l, buffer_lr, (M_right_to_left, M_left_to_right), (M_left_right_left, M_right_left_right), \\\n","            (V_left_to_right, V_right_to_left) = self.pam(res1_l, res2_l, is_training)\n","\n","            # print(\"buffer_l shape after 2nd PAM left:\", buffer_l.shape)\n","             ### parallax attention_right\n","            buffer_rl, buffer_r, (M_right_to_left, M_left_to_right), (M_left_right_left, M_right_left_right), \\\n","            (V_left_to_right, V_right_to_left) = self.pam(res1_r, res2_r, is_training)\n","\n","            buffer_l = self.Dblocks(buffer_l)\n","            buffer_r = self.Dblocks(buffer_r)\n","\n","            ### creating anchors\n","\n","            x_left_anch = self.anch_func([x_left] * 10)\n","            x_right_anch = self.anch_func([x_right] * 10)\n","            x_left_anch = self.conv_anch(x_left_anch)\n","            x_right_anch = self.conv_anch(x_right_anch)\n","\n","            ### upscaling\n","\n","            buffer_l = self.conv2(buffer_l)\n","            buffer_r = self.conv2(buffer_r)\n","\n","            x_left_anch = self.conv2(x_left_anch)\n","            x_right_anch = self.conv2(x_right_anch)\n","\n","            buffer_l = self.clippedReLU(buffer_l)\n","            buffer_r = self.clippedReLU(buffer_r)\n","\n","            x_left_anch = self.clippedReLU(x_left_anch)\n","            x_right_anch = self.clippedReLU(x_right_anch)\n","\n","            # print(\"buffer_l shape:\", buffer_l.shape)\n","            # print(\"x_left_anch shape:\", x_left_anch.shape)\n","\n","            out_l = buffer_l + x_left_anch\n","            out_l = self.upscale(out_l)\n","\n","            out_r = buffer_r + x_right_anch\n","            out_r = self.upscale(out_r)\n","\n","            x_left_up = F.interpolate(x_left, scale_factor=self.upscale_factor, mode='bicubic', align_corners=False)\n","            x_right_up = F.interpolate(x_right, scale_factor=self.upscale_factor, mode='bicubic', align_corners=False)\n","\n","            out_l = x_left_up + out_l\n","            out_r = x_right_up + out_r\n","\n","            out_l = torch.clamp(out_l, 0., 255.)\n","            out_r = torch.clamp(out_r, 0., 255.)\n","\n","            return out_l, out_r, (M_right_to_left, M_left_to_right), (M_left_right_left, M_right_left_right), \\\n","                   (V_left_to_right, V_right_to_left)\n","\n","        if is_training == 0:\n","            ### parallax attention_1\n","            buffer_lt, buffer_rt = self.pam(buffer_left, buffer_right, is_training)\n","\n","            # print(\"PAM 1st PAM shape:\", buffer.shape)\n","\n","            ##### G-block\n","            res1_l = self.Dblocks(buffer_lt)\n","            res1_r = self.Dblocks(buffer_rt)\n","\n","            # print(\"o/p of Dblock shape:\", res1.shape)\n","\n","            res2_l = self.conv1(x_left)\n","            res2_r = self.conv1(x_right)\n","\n","            # print(\"res2_l shape:\", res2_l.shape)\n","            # print(\"res2_r shape:\", res2_r.shape)\n","\n","            ### parallax attention_2\n","            buffer_l, buffer_lr = self.pam(res1_l, res2_l, is_training)\n","            buffer_rl, buffer_r = self.pam(res1_r, res2_r, is_training)\n","\n","            buffer_l = self.Dblocks(buffer_l)\n","            buffer_r = self.Dblocks(buffer_r)\n","\n","             ### creating anchors\n","\n","            x_left_anch = self.anch_func([x_left] * 10)\n","            x_right_anch = self.anch_func([x_right] * 10)\n","            x_left_anch = self.conv_anch(x_left_anch)\n","            x_right_anch = self.conv_anch(x_right_anch)\n","\n","            ### upscaling\n","\n","            buffer_l = self.conv2(buffer_l)\n","            buffer_r = self.conv2(buffer_r)\n","\n","            x_left_anch = self.conv2(x_left_anch)\n","            x_right_anch = self.conv2(x_right_anch)\n","\n","            buffer_l = self.clippedReLU(buffer_l)\n","            buffer_r = self.clippedReLU(buffer_r)\n","\n","            x_left_anch = self.clippedReLU(x_left_anch)\n","            x_right_anch = self.clippedReLU(x_right_anch)\n","\n","            # print(\"buffer_l shape:\", buffer_l.shape)\n","            # print(\"x_left_anch shape:\", x_left_anch.shape)\n","\n","            out_l = buffer_l + x_left_anch\n","            out_l = self.upscale(out_l)\n","\n","            out_r = buffer_r + x_right_anch\n","            out_r = self.upscale(out_r)\n","\n","            x_left_up = F.interpolate(x_left, scale_factor=self.upscale_factor, mode='bicubic', align_corners=False)\n","            x_right_up = F.interpolate(x_right, scale_factor=self.upscale_factor, mode='bicubic', align_corners=False)\n","\n","            out_l = x_left_up + out_l\n","            out_r = x_right_up + out_r\n","\n","            out_l = torch.clamp(out_l, 0., 255.)\n","            out_r = torch.clamp(out_r, 0., 255.)\n","\n","            return out_l, out_r\n","\n","\n","if __name__ == '__main__':\n","    device = 'cuda'  # Change to 'cpu' if you don't have a GPU\n","    # device = 'cpu'\n","    model = XLSR_stereo_AM(4).to(device)\n","    model.eval()\n","\n","    # # Create random left and right low-resolution stereo images (batch size = 1)\n","    # left_image = torch.randn(1, 3, 32, 32).to(device)\n","    # right_image = torch.randn(1, 3, 32, 32).to(device)\n","\n","    # # Forward pass through the network\n","    # pred = model(left_image, right_image, is_training=1)\n","\n","\n","    # # Unpack the elements of the prediction tuple\n","    # if len(pred) == 5:  # Assuming the tuple contains 5 elements as per your model definition\n","    #     HR_pred_l, HR_pred_r, (M_right_to_left, M_left_to_right), (M_left_right_left, M_right_left_right), \\\n","    #      (V_left_to_right, V_right_to_left) = pred\n","    # elif len(pred) == 2:\n","    #     HR_pred_l, HR_pred_r = pred\n","\n","\n","\n","pytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n","print(\"Total parameters =\", pytorch_total_params)\n","\n","# Print the model summary\n","# summary(model, input_size=[(3, 32, 32), (3, 32, 32)])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":373},"id":"SwS4a-OBhbMA","outputId":"0fd9389b-26a2-41b8-964a-d306f066ea7c","executionInfo":{"status":"error","timestamp":1714377039762,"user_tz":-330,"elapsed":10884,"user":{"displayName":"SmG Savitri","userId":"12776258865114306376"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Loaded pretrained model /content/drive/MyDrive/phd/wk1/phase1_baseline/Output_UP_VM_CR_b4_loss_anchor/x2/output1/best.pt\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-7d6e93c004b9>\u001b[0m in \u001b[0;36m<cell line: 201>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m train_dataloader = create_dataloader('train', SR_rate, augment, batch_size,\n\u001b[0m\u001b[1;32m    202\u001b[0m                                      shuffle=True, num_workers=workers)\n\u001b[1;32m    203\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-2-31fe863e360c>\u001b[0m in \u001b[0;36mcreate_dataloader\u001b[0;34m(split, SR_rate, augment, batch_size, shuffle, num_workers, pin_memory)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcreate_dataloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSR_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maugment\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpin_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSR_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maugment\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mdataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_workers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpin_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# print('check 0', dataloader) #me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-2-31fe863e360c>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, split, SR_rate, augment)\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHR_dir_l\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/MyDrive/phd/StereoSR/datasets/Flickr_modifd/HR/size_corrected/hr_x2/train/left'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHR_dir_r\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/MyDrive/phd/StereoSR/datasets/Flickr_modifd/HR/size_corrected/hr_x2/train/right'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg_names_l\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLR_dir_l\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg_names_r\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLR_dir_r\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0;31m# print('Sorted HR_l images upto 5: ', self.img_names_l)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["# # train.py\n","\n","# import os\n","# import yaml\n","# import argparse\n","# import torch\n","# from torch.autograd import Variable\n","# from torch.utils.data import DataLoader\n","# import torch.backends.cudnn as cudnn\n","# import time\n","# import torch.optim as optim\n","# import torch.optim.lr_scheduler as lr_scheduler\n","# import pandas as pd\n","\n","\n","# # flag = True # for storing checkpoints\n","# # CP = True   # for storing checkpoints\n","\n","\n","\n","# def train(model, dataloader, criteria, device, optimizer, scheduler):\n","#     loss_epoch = 0.\n","#     criterion_L1 = L1Loss()\n","#     for LR_img_l, LR_img_r,HR_img_l, HR_img_r, _ , _ in dataloader:\n","\n","#         optimizer.zero_grad()\n","#         # LR_img_l, LR_img_r = LR_img_l.to(device).float(), LR_img_r.to(device).float()\n","#         # HR_img_l, HR_img_r = HR_img_l.to(device).float(), HR_img_r.to(device).float()\n","#         LR_left, LR_right = LR_img_l.to(device).float(), LR_img_r.to(device).float()\n","#         HR_left, HR_right = HR_img_l.to(device).float(), HR_img_r.to(device).float()\n","#         # HR_pred_l, HR_pred_r, (M_right_to_left, M_left_to_right), (M_left_right_left, M_right_left_right), \\\n","#         #  (V_left_to_right, V_right_to_left) = model(LR_img_l, LR_img_r, is_training = 1)\n","\n","#         SR_left, SR_right, (M_right_to_left, M_left_to_right), (M_left_right_left, M_right_left_right), \\\n","#          (V_left, V_right)= model(LR_left, LR_right, is_training = 1)\n","#         b, c, h, w = LR_left.shape\n","\n","#         # loss_l = criteria(HR_pred_l, HR_img_l)\n","#         # loss_r = criteria(HR_pred_r, HR_img_r)\n","\n","#         loss_SR = criteria(SR_left, HR_left) + criteria(SR_right, HR_right)\n","\n","#         # ### loss_smoothness\n","#         # loss_h = criterion_L1(M_right_to_left[:, :-1, :, :], M_right_to_left[:, 1:, :, :]) + \\\n","#         #            criterion_L1(M_left_to_right[:, :-1, :, :], M_left_to_right[:, 1:, :, :])\n","#         # loss_w = criterion_L1(M_right_to_left[:, :, :-1, :-1], M_right_to_left[:, :, 1:, 1:]) + \\\n","#         #            criterion_L1(M_left_to_right[:, :, :-1, :-1], M_left_to_right[:, :, 1:, 1:])\n","#         # loss_smooth = loss_w + loss_h\n","\n","#         # ### loss_cycle\n","#         # Identity = Variable(torch.eye(w, w).repeat(b, h, 1, 1), requires_grad=False).to(device)\n","#         # loss_cycle = criterion_L1(M_left_right_left * V_left_to_right.permute(0, 2, 1, 3), Identity * V_left_to_right.permute(0, 2, 1, 3)) + \\\n","#         #                  criterion_L1(M_right_left_right * V_right_to_left.permute(0, 2, 1, 3), Identity * V_right_to_left.permute(0, 2, 1, 3))\n","\n","#         # ### loss_photometric\n","#         # LR_right_warped = torch.bmm(M_right_to_left.contiguous().view(b*h,w,w), LR_img_r.permute(0,2,3,1).contiguous().view(b*h, w, c))\n","#         # LR_right_warped = LR_right_warped.view(b, h, w, c).contiguous().permute(0, 3, 1, 2)\n","#         # LR_left_warped = torch.bmm(M_left_to_right.contiguous().view(b * h, w, w), LR_img_l.permute(0, 2, 3, 1).contiguous().view(b * h, w, c))\n","#         # LR_left_warped = LR_left_warped.view(b, h, w, c).contiguous().permute(0, 3, 1, 2)\n","\n","#         # loss_photo = criterion_L1(LR_img_l * V_left_to_right, LR_right_warped * V_left_to_right) + \\\n","#         #                   criterion_L1(LR_img_r * V_right_to_left, LR_left_warped * V_right_to_left)\n","\n","#         # ### losses\n","#         # loss = loss_SR + 0.005 * (loss_photo + loss_smooth + loss_cycle)\n","\n","#         ''' Photometric Loss '''\n","#         Res_left = torch.abs(HR_left - F.interpolate(LR_left, scale_factor=scale, mode='bicubic', align_corners=False))\n","#         Res_left = F.interpolate(Res_left, scale_factor=1 / scale, mode='bicubic', align_corners=False)\n","#         Res_right = torch.abs(HR_right - F.interpolate(LR_right, scale_factor=scale, mode='bicubic', align_corners=False))\n","#         Res_right = F.interpolate(Res_right, scale_factor=1 / scale, mode='bicubic', align_corners=False)\n","#         Res_leftT = torch.bmm(M_right_to_left.contiguous().view(b * h, w, w), Res_right.permute(0, 2, 3, 1).contiguous().view(b * h, w, c)\n","#                                   ).view(b, h, w, c).contiguous().permute(0, 3, 1, 2)\n","#         Res_rightT = torch.bmm(M_left_to_right.contiguous().view(b * h, w, w), Res_left.permute(0, 2, 3, 1).contiguous().view(b * h, w, c)\n","#                                    ).view(b, h, w, c).contiguous().permute(0, 3, 1, 2)\n","#         loss_photo = criterion_L1(Res_left * V_left.repeat(1, 3, 1, 1), Res_leftT * V_left.repeat(1, 3, 1, 1)) + \\\n","#                          criterion_L1(Res_right * V_right.repeat(1, 3, 1, 1), Res_rightT * V_right.repeat(1, 3, 1, 1))\n","\n","#         ''' Smoothness Loss '''\n","#         loss_h = criterion_L1(M_right_to_left[:, :-1, :, :], M_right_to_left[:, 1:, :, :]) + \\\n","#                      criterion_L1(M_left_to_right[:, :-1, :, :], M_left_to_right[:, 1:, :, :])\n","#         loss_w = criterion_L1(M_right_to_left[:, :, :-1, :-1], M_right_to_left[:, :, 1:, 1:]) + \\\n","#                      criterion_L1(M_left_to_right[:, :, :-1, :-1], M_left_to_right[:, :, 1:, 1:])\n","#         loss_smooth = loss_w + loss_h\n","\n","#         ''' Cycle Loss '''\n","#         Res_left_cycle = torch.bmm(M_right_to_left.contiguous().view(b * h, w, w), Res_rightT.permute(0, 2, 3, 1).contiguous().view(b * h, w, c)\n","#                                        ).view(b, h, w, c).contiguous().permute(0, 3, 1, 2)\n","#         Res_right_cycle = torch.bmm(M_left_to_right.contiguous().view(b * h, w, w), Res_leftT.permute(0, 2, 3, 1).contiguous().view(b * h, w, c)\n","#                                         ).view(b, h, w, c).contiguous().permute(0, 3, 1, 2)\n","#         loss_cycle = criterion_L1(Res_left * V_left.repeat(1, 3, 1, 1), Res_left_cycle * V_left.repeat(1, 3, 1, 1)) + \\\n","#                          criterion_L1(Res_right * V_right.repeat(1, 3, 1, 1), Res_right_cycle * V_right.repeat(1, 3, 1, 1))\n","\n","#         ''' Consistency Loss '''\n","#         SR_left_res = F.interpolate(torch.abs(HR_left - SR_left), scale_factor=1 / scale, mode='bicubic', align_corners=False)\n","#         SR_right_res = F.interpolate(torch.abs(HR_right - SR_right), scale_factor=1 / scale, mode='bicubic', align_corners=False)\n","#         SR_left_resT = torch.bmm(M_right_to_left.detach().contiguous().view(b * h, w, w), SR_right_res.permute(0, 2, 3, 1).contiguous().view(b * h, w, c)\n","#                                      ).view(b, h, w, c).contiguous().permute(0, 3, 1, 2)\n","#         SR_right_resT = torch.bmm(M_left_to_right.detach().contiguous().view(b * h, w, w), SR_left_res.permute(0, 2, 3, 1).contiguous().view(b * h, w, c)\n","#                                       ).view(b, h, w, c).contiguous().permute(0, 3, 1, 2)\n","#         loss_cons = criterion_L1(SR_left_res * V_left.repeat(1, 3, 1, 1), SR_left_resT * V_left.repeat(1, 3, 1, 1)) + \\\n","#                        criterion_L1(SR_right_res * V_right.repeat(1, 3, 1, 1), SR_right_resT * V_right.repeat(1, 3, 1, 1))\n","\n","#         ''' Total Loss '''\n","#         loss = loss_SR + 0.1 * loss_cons + 0.1 * (loss_photo + loss_smooth + loss_cycle)\n","\n","\n","#         # Backpropagation\n","#         loss.backward()\n","#         optimizer.step()\n","#         scheduler.step()\n","#         # print(\"start_step:\", scheduler.last_epoch)   # me\n","#         # print(\"end_step:\", scheduler.end_step)       #me\n","#         loss_epoch += loss.item()\n","#     loss_epoch /= len(dataloader)\n","#     lr_epoch = scheduler.get_last_lr()[0]\n","#     scheduler.step() # me\n","#     return loss_epoch, lr_epoch\n","\n","# def validation(model, dataloader, criteria, device):\n","#     loss_epoch = 0.\n","#     psnr_epoch = 0.\n","#     pred_list_l = []\n","#     pred_list_r = []\n","#     name_list_l = []\n","#     name_list_r = []\n","#     with torch.no_grad():\n","#         for LR_img_l, LR_img_r, HR_img_l, HR_img_r, img_name_l, img_name_r in dataloader:\n","#             LR_img_l, LR_img_r = LR_img_l.to(device).float(), LR_img_r.to(device).float()\n","#             HR_img_l , HR_img_r =  HR_img_l.to(device).float(), HR_img_r.to(device).float()\n","\n","#             HR_pred_l, HR_pred_r = model(LR_img_l, LR_img_r, is_training=0)\n","#             SR_left = torch.clamp(HR_pred_l, 0, 1)\n","\n","#             # HR_pred_l, HR_pred_r = HR_pred[:, :3, :, :], HR_pred[:, 3:, :, :]\n","#             loss_l = criteria(HR_pred_l, HR_img_l)\n","#             # loss_r = criteria(HR_pred_r, HR_img_r)\n","#             loss = loss_l\n","#             loss_epoch += loss.item()\n","#             psnr_epoch_l = cal_psnr(HR_pred_l, HR_img_l)\n","#             # psnr_epoch_r = cal_psnr(HR_pred_r, HR_img_r)\n","#             psnr_batch = psnr_epoch_l\n","#             psnr_epoch += psnr_batch.item()\n","#             pred_list_l.append(HR_pred_l)\n","#             pred_list_r.append(HR_pred_r)\n","#             name_list_l += img_name_l\n","#             name_list_r += img_name_r\n","#     loss_epoch /= len(dataloader)\n","#     psnr_epoch /= len(dataloader)\n","#     return loss_epoch, psnr_epoch, pred_list_l, pred_list_r, name_list_l, name_list_r\n","\n","# save_dir = \"/content/drive/MyDrive/phd/wk1/phase1_baseline/Output_UP_VM_CR_b4_loss_anchor/x2/output1\"\n","# SR_rate = 2\n","# pretrained_model = \"/content/drive/MyDrive/phd/wk1/phase1_baseline/Output_UP_VM_CR_b4_loss_anchor/x2/output1/best.pt\"\n","# # epochs = 5000\n","# epochs = 500\n","# batch_size = 16\n","# lr_max = 25e-04\n","# pct_epoch = 30\n","# augment = 'store_true'\n","# workers = 2\n","# device = 0\n","# div_factor = 50.0\n","# final_div_factor = 0.5\n","\n","# scale = SR_rate\n","# # if os.path.exists(save_dir):\n","# #         print(f\"Warning: {save_dir} exists, please delete it manually if it is useless.\")\n","\n","\n","# # txt file to record training process in EXCEL\n","# # results_df = pd.DataFrame(columns=['Epoch', 'Learning Rate', 'Training Loss',\n","# #                                    'Validation Loss', 'PSNR', 'Time'])\n","# # txt_path = os.path.join(save_dir, 'results.txt')\n","# # if os.path.exists(txt_path):\n","# #         os.remove(txt_path)\n","\n","# # folder to save the predicted HR image in the validation\n","# valid_folder = os.path.join(save_dir, 'valid_res')\n","# os.makedirs(valid_folder, exist_ok=True)\n","\n","# device = 'cuda'\n","# # device = 'cpu'\n","\n","# model = XLSR_stereo_AM(SR_rate)\n","# #model = PTSQ_quantized_model(opt[1])\n","\n","# # load pretrained model        ########################## uncomment if traing needed a restart\n","# if pretrained_model.endswith('.pt') and os.path.exists(pretrained_model):\n","#         # filter conv4 weights which have different conv channels\n","#         model_dict = model.state_dict()\n","#         pretrained_dict = torch.load(pretrained_model)\n","#         filtered_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict and v.shape == model_dict[k].shape}\n","#         model_dict.update(filtered_dict)\n","#         model.load_state_dict(model_dict)\n","#         print(f\"Loaded pretrained model {pretrained_model}\" )\n","\n","\n","# model.to(device)\n","\n","# train_dataloader = create_dataloader('train', SR_rate, augment, batch_size,\n","#                                      shuffle=True, num_workers=workers)\n","# batch = next(iter(train_dataloader))\n","\n","\n","# valid_dataloader = create_dataloader('valid', SR_rate, False, 1,\n","#                                      shuffle=True, num_workers=1)\n","\n","# criteria = CharbonnierLoss()\n","# optimizer = optim.Adam(model.parameters(), lr=lr_max/div_factor, betas=(0.9, 0.999), eps=1e-08)\n","\n","\n","# scheduler = lr_scheduler.OneCycleLR(optimizer, lr_max, epochs=epochs, steps_per_epoch=len(train_dataloader), pct_start=pct_epoch/epochs, anneal_strategy='cos', \\\n","#                                         cycle_momentum=False, div_factor=div_factor, final_div_factor=final_div_factor)\n","\n","\n","# epoch_start = 487\n","# txt_path = os.path.join(save_dir, f\"result_epoch_{epoch_start}.txt\")\n","# best_psnr = 0.\n","# for idx in range(epoch_start, epochs+1):\n","#         t0 = time.time()\n","#         train_loss_epoch, lr_epoch = train(model, train_dataloader, criteria, device, optimizer, scheduler)\n","#         t1 = time.time()\n","#         valid_loss_epoch, psnr_epoch, pred_HR_l, pred_HR_r, img_names_l, img_names_r = validation(model, valid_dataloader, criteria, device)\n","#         t2 = time.time()\n","#         print(f\"Epoch: {idx} | lr: {lr_epoch:.5f} | training loss: {train_loss_epoch:.5f} | validation loss: {valid_loss_epoch:.5f} | PSNR: {psnr_epoch:.3f} | Time: {t2-t0:.1f}\")\n","#         with open(txt_path, 'a') as f:\n","#             f.write(f\"Epoch: {idx} | lr: {lr_epoch:.5f} | training loss: {train_loss_epoch:.5f} | validation loss: {valid_loss_epoch:.5f} | PSNR: {psnr_epoch:.3f} | Time: {t2-t0:.1f}\" +'\\n')\n","\n","#         if psnr_epoch > best_psnr:\n","#             best_psnr = psnr_epoch\n","\n","#             torch.save(model.state_dict(), os.path.join(save_dir, 'best.pt'))\n","#             # save predicted HR image on validation set\n","#             save_res(pred_HR_l,pred_HR_r, img_names_l,img_names_r, valid_folder)\n","#         del pred_HR_l, pred_HR_r\n","\n","\n","# #  # Save the DataFrame to an Excel file after the loop\n","# # excel_file_path = os.path.join(save_dir, 'results.xlsx')\n","# # results_df.to_excel(excel_file_path, index=False)\n","\n","# #  # visualize the training process\n","# # visualize_training(save_dir)\n","# # print(f\"Training is finished, the best PSNR is {best_psnr:.3f}\")\n","# # with open(excel_file_path, 'a') as f:\n","# #             f.write(f\"Training is finished, the best PSNR is {best_psnr:.3f}\")\n","\n","#  # visualize the training process\n","# visualize_training(save_dir, epoch_start)\n","# print(f\"Training is finished, the best PSNR is {best_psnr:.3f}\")\n","# with open(txt_path, 'a') as f:\n","#             f.write(f\"Training is finished, the best PSNR is {best_psnr:.3f}\")\n","\n"]},{"cell_type":"code","source":["#computing the Structural SImilarity Index Metrix\n","\n","#luminance computation\n","def l_x_y(x,y,C1):\n","    mean_x = torch.mean(x)\n","    mean_y = torch.mean(y)\n","    numerator = (2*mean_x*mean_y) + (C1)\n","    denominator = (mean_x**2)+(mean_y**2)+C1\n","    return (numerator/denominator)\n","#contrast computation\n","def c_x_y(x,y,C2):\n","    std_x = torch.std(x,unbiased=True)\n","    std_y = torch.std(y,unbiased=True)\n","    numerator = (2*std_x*std_y) + C2\n","    denominator = (std_x**2) + (std_y**2) + C2\n","    return (numerator/denominator)\n","#structure of the image computation\n","def s_x_y(x,y,C3):\n","    mean_x = torch.mean(x)\n","    mean_y = torch.mean(y)\n","    std_x = torch.std(x,unbiased=True)\n","    std_y = torch.std(y,unbiased=True)\n","    x_ = x - mean_x\n","    y_ = x - mean_y\n","    # sigma_x_y = torch.sum(x_ * y_)/((np.shape(x)[0]*np.shape(x)[1])-1)\n","    sigma_x_y = torch.sum(x_ * y_) / (torch.numel(x) - 1)  # Corrected calculation\n","    numerator = sigma_x_y + C3\n","    denominator = (std_x*std_y) + C3\n","    return (numerator/denominator)\n","\n","#computing SSIM\n","def compute_SSIM(image_1,image_2):\n","    l_exp = 1\n","    c_exp = 1\n","    s_exp = 1\n","    l_ = l_x_y(image_1,image_2,0.00001)\n","    c_ = c_x_y(image_1,image_2,0.00001)\n","    s_ = s_x_y(image_1,image_2,0.00001)\n","    l = l_**l_exp\n","    c = c_**c_exp\n","    s = s_**s_exp\n","    return (l*c*s)"],"metadata":{"id":"KcNhPAInlChK","executionInfo":{"status":"ok","timestamp":1716459088941,"user_tz":-330,"elapsed":412,"user":{"displayName":"SmG Savitri","userId":"12776258865114306376"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","execution_count":8,"metadata":{"id":"4DCLwE2ciGW8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1716459220195,"user_tz":-330,"elapsed":128749,"user":{"displayName":"SmG Savitri","userId":"12776258865114306376"}},"outputId":"3af9eb35-549c-480c-fd9b-dba768340b62"},"outputs":[{"output_type":"stream","name":"stdout","text":["112\n","Start the inference ...\n","PSRN on ['0001_L.png'] and ['0001_R.png'] : 28.921, inference time: 2806.44ms, SSIM = 1.0\n","PSRN on ['0002_L.png'] and ['0002_R.png'] : 33.248, inference time: 508.30ms, SSIM = 1.0\n","PSRN on ['0003_L.png'] and ['0003_R.png'] : 25.835, inference time: 410.85ms, SSIM = 0.9\n","PSRN on ['0004_L.png'] and ['0004_R.png'] : 25.495, inference time: 1536.78ms, SSIM = 1.0\n","PSRN on ['0005_L.png'] and ['0005_R.png'] : 33.307, inference time: 448.05ms, SSIM = 1.0\n","PSRN on ['0006_L.png'] and ['0006_R.png'] : 31.225, inference time: 307.54ms, SSIM = 1.0\n","PSRN on ['0007_L.png'] and ['0007_R.png'] : 34.040, inference time: 63.39ms, SSIM = 1.0\n","PSRN on ['0008_L.png'] and ['0008_R.png'] : 33.418, inference time: 84.64ms, SSIM = 1.0\n","PSRN on ['0009_L.png'] and ['0009_R.png'] : 22.566, inference time: 356.15ms, SSIM = 0.9\n","PSRN on ['0010_L.png'] and ['0010_R.png'] : 23.040, inference time: 500.86ms, SSIM = 1.0\n","PSRN on ['0011_L.png'] and ['0011_R.png'] : 23.579, inference time: 96.24ms, SSIM = 0.9\n","PSRN on ['0012_L.png'] and ['0012_R.png'] : 35.785, inference time: 87.30ms, SSIM = 1.0\n","PSRN on ['0013_L.png'] and ['0013_R.png'] : 32.197, inference time: 641.02ms, SSIM = 1.0\n","PSRN on ['0014_L.png'] and ['0014_R.png'] : 21.267, inference time: 407.29ms, SSIM = 0.9\n","PSRN on ['0015_L.png'] and ['0015_R.png'] : 26.946, inference time: 465.92ms, SSIM = 1.0\n","PSRN on ['0016_L.png'] and ['0016_R.png'] : 23.808, inference time: 434.99ms, SSIM = 1.0\n","PSRN on ['0017_L.png'] and ['0017_R.png'] : 26.298, inference time: 68.40ms, SSIM = 1.0\n","PSRN on ['0018_L.png'] and ['0018_R.png'] : 30.323, inference time: 452.82ms, SSIM = 1.0\n","PSRN on ['0019_L.png'] and ['0019_R.png'] : 29.140, inference time: 94.50ms, SSIM = 1.0\n","PSRN on ['0020_L.png'] and ['0020_R.png'] : 32.935, inference time: 551.01ms, SSIM = 1.0\n","PSRN on ['0021_L.png'] and ['0021_R.png'] : 34.772, inference time: 626.53ms, SSIM = 1.0\n","PSRN on ['0022_L.png'] and ['0022_R.png'] : 32.186, inference time: 85.27ms, SSIM = 1.0\n","PSRN on ['0023_L.png'] and ['0023_R.png'] : 24.519, inference time: 444.45ms, SSIM = 1.0\n","PSRN on ['0024_L.png'] and ['0024_R.png'] : 27.151, inference time: 101.48ms, SSIM = 1.0\n","PSRN on ['0025_L.png'] and ['0025_R.png'] : 26.374, inference time: 476.29ms, SSIM = 1.0\n","PSRN on ['0026_L.png'] and ['0026_R.png'] : 32.934, inference time: 404.46ms, SSIM = 1.0\n","PSRN on ['0027_L.png'] and ['0027_R.png'] : 32.435, inference time: 73.16ms, SSIM = 1.0\n","PSRN on ['0028_L.png'] and ['0028_R.png'] : 31.129, inference time: 425.75ms, SSIM = 1.0\n","PSRN on ['0029_L.png'] and ['0029_R.png'] : 30.829, inference time: 106.34ms, SSIM = 1.0\n","PSRN on ['0030_L.png'] and ['0030_R.png'] : 24.133, inference time: 106.38ms, SSIM = 0.9\n","PSRN on ['0031_L.png'] and ['0031_R.png'] : 37.289, inference time: 114.36ms, SSIM = 1.0\n","PSRN on ['0032_L.png'] and ['0032_R.png'] : 41.147, inference time: 72.03ms, SSIM = 1.0\n","PSRN on ['0033_L.png'] and ['0033_R.png'] : 19.235, inference time: 162.72ms, SSIM = 0.9\n","PSRN on ['0034_L.png'] and ['0034_R.png'] : 18.968, inference time: 392.05ms, SSIM = 0.9\n","PSRN on ['0035_L.png'] and ['0035_R.png'] : 27.270, inference time: 85.52ms, SSIM = 1.0\n","PSRN on ['0036_L.png'] and ['0036_R.png'] : 25.051, inference time: 101.18ms, SSIM = 1.0\n","PSRN on ['0037_L.png'] and ['0037_R.png'] : 25.641, inference time: 143.61ms, SSIM = 1.0\n","PSRN on ['0038_L.png'] and ['0038_R.png'] : 34.603, inference time: 532.69ms, SSIM = 1.0\n","PSRN on ['0039_L.png'] and ['0039_R.png'] : 23.591, inference time: 89.70ms, SSIM = 1.0\n","PSRN on ['0040_L.png'] and ['0040_R.png'] : 21.958, inference time: 86.73ms, SSIM = 0.9\n","PSRN on ['0041_L.png'] and ['0041_R.png'] : 38.682, inference time: 76.42ms, SSIM = 1.0\n","PSRN on ['0042_L.png'] and ['0042_R.png'] : 27.547, inference time: 71.03ms, SSIM = 1.0\n","PSRN on ['0043_L.png'] and ['0043_R.png'] : 28.300, inference time: 162.96ms, SSIM = 1.0\n","PSRN on ['0044_L.png'] and ['0044_R.png'] : 29.492, inference time: 70.10ms, SSIM = 1.0\n","PSRN on ['0045_L.png'] and ['0045_R.png'] : 33.178, inference time: 384.31ms, SSIM = 1.0\n","PSRN on ['0046_L.png'] and ['0046_R.png'] : 25.398, inference time: 42.25ms, SSIM = 1.0\n","PSRN on ['0047_L.png'] and ['0047_R.png'] : 38.271, inference time: 102.80ms, SSIM = 1.0\n","PSRN on ['0048_L.png'] and ['0048_R.png'] : 31.927, inference time: 88.77ms, SSIM = 1.0\n","PSRN on ['0049_L.png'] and ['0049_R.png'] : 25.762, inference time: 100.28ms, SSIM = 1.0\n","PSRN on ['0050_L.png'] and ['0050_R.png'] : 22.424, inference time: 71.17ms, SSIM = 1.0\n","PSRN on ['0051_L.png'] and ['0051_R.png'] : 26.854, inference time: 63.99ms, SSIM = 1.0\n","PSRN on ['0052_L.png'] and ['0052_R.png'] : 30.776, inference time: 93.58ms, SSIM = 1.0\n","PSRN on ['0053_L.png'] and ['0053_R.png'] : 24.626, inference time: 104.38ms, SSIM = 1.0\n","PSRN on ['0054_L.png'] and ['0054_R.png'] : 24.743, inference time: 162.66ms, SSIM = 1.0\n","PSRN on ['0055_L.png'] and ['0055_R.png'] : 26.180, inference time: 538.17ms, SSIM = 0.9\n","PSRN on ['0056_L.png'] and ['0056_R.png'] : 31.943, inference time: 100.14ms, SSIM = 1.0\n","PSRN on ['0057_L.png'] and ['0057_R.png'] : 23.953, inference time: 105.75ms, SSIM = 1.0\n","PSRN on ['0058_L.png'] and ['0058_R.png'] : 34.007, inference time: 95.35ms, SSIM = 1.0\n","PSRN on ['0059_L.png'] and ['0059_R.png'] : 28.994, inference time: 87.04ms, SSIM = 1.0\n","PSRN on ['0060_L.png'] and ['0060_R.png'] : 35.663, inference time: 125.65ms, SSIM = 1.0\n","PSRN on ['0061_L.png'] and ['0061_R.png'] : 25.047, inference time: 49.80ms, SSIM = 1.0\n","PSRN on ['0062_L.png'] and ['0062_R.png'] : 31.369, inference time: 97.03ms, SSIM = 1.0\n","PSRN on ['0063_L.png'] and ['0063_R.png'] : 37.070, inference time: 105.62ms, SSIM = 1.0\n","PSRN on ['0064_L.png'] and ['0064_R.png'] : 37.332, inference time: 385.16ms, SSIM = 1.0\n","PSRN on ['0065_L.png'] and ['0065_R.png'] : 22.517, inference time: 107.06ms, SSIM = 0.9\n","PSRN on ['0066_L.png'] and ['0066_R.png'] : 27.485, inference time: 105.65ms, SSIM = 1.0\n","PSRN on ['0067_L.png'] and ['0067_R.png'] : 34.256, inference time: 64.10ms, SSIM = 1.0\n","PSRN on ['0068_L.png'] and ['0068_R.png'] : 30.001, inference time: 75.73ms, SSIM = 1.0\n","PSRN on ['0069_L.png'] and ['0069_R.png'] : 21.216, inference time: 176.06ms, SSIM = 0.9\n","PSRN on ['0070_L.png'] and ['0070_R.png'] : 28.456, inference time: 99.49ms, SSIM = 1.0\n","PSRN on ['0071_L.png'] and ['0071_R.png'] : 25.832, inference time: 89.74ms, SSIM = 1.0\n","PSRN on ['0072_L.png'] and ['0072_R.png'] : 27.130, inference time: 64.56ms, SSIM = 1.0\n","PSRN on ['0073_L.png'] and ['0073_R.png'] : 24.930, inference time: 163.06ms, SSIM = 1.0\n","PSRN on ['0074_L.png'] and ['0074_R.png'] : 26.270, inference time: 100.19ms, SSIM = 1.0\n","PSRN on ['0075_L.png'] and ['0075_R.png'] : 24.870, inference time: 459.74ms, SSIM = 1.0\n","PSRN on ['0076_L.png'] and ['0076_R.png'] : 21.662, inference time: 49.43ms, SSIM = 1.0\n","PSRN on ['0077_L.png'] and ['0077_R.png'] : 30.329, inference time: 40.73ms, SSIM = 1.0\n","PSRN on ['0078_L.png'] and ['0078_R.png'] : 25.111, inference time: 64.69ms, SSIM = 1.0\n","PSRN on ['0079_L.png'] and ['0079_R.png'] : 42.931, inference time: 65.18ms, SSIM = 1.0\n","PSRN on ['0080_L.png'] and ['0080_R.png'] : 24.778, inference time: 553.59ms, SSIM = 1.0\n","PSRN on ['0081_L.png'] and ['0081_R.png'] : 26.992, inference time: 444.04ms, SSIM = 1.0\n","PSRN on ['0082_L.png'] and ['0082_R.png'] : 32.569, inference time: 43.18ms, SSIM = 1.0\n","PSRN on ['0083_L.png'] and ['0083_R.png'] : 23.429, inference time: 35.12ms, SSIM = 1.0\n","PSRN on ['0084_L.png'] and ['0084_R.png'] : 31.340, inference time: 54.52ms, SSIM = 1.0\n","PSRN on ['0085_L.png'] and ['0085_R.png'] : 33.324, inference time: 85.92ms, SSIM = 1.0\n","PSRN on ['0086_L.png'] and ['0086_R.png'] : 30.612, inference time: 60.02ms, SSIM = 1.0\n","PSRN on ['0087_L.png'] and ['0087_R.png'] : 32.646, inference time: 56.31ms, SSIM = 1.0\n","PSRN on ['0088_L.png'] and ['0088_R.png'] : 30.339, inference time: 512.04ms, SSIM = 1.0\n","PSRN on ['0089_L.png'] and ['0089_R.png'] : 30.681, inference time: 39.57ms, SSIM = 1.0\n","PSRN on ['0090_L.png'] and ['0090_R.png'] : 29.322, inference time: 483.61ms, SSIM = 1.0\n","PSRN on ['0091_L.png'] and ['0091_R.png'] : 32.992, inference time: 42.24ms, SSIM = 1.0\n","PSRN on ['0092_L.png'] and ['0092_R.png'] : 28.798, inference time: 41.69ms, SSIM = 1.0\n","PSRN on ['0093_L.png'] and ['0093_R.png'] : 35.059, inference time: 65.00ms, SSIM = 1.0\n","PSRN on ['0094_L.png'] and ['0094_R.png'] : 24.428, inference time: 63.79ms, SSIM = 1.0\n","PSRN on ['0095_L.png'] and ['0095_R.png'] : 25.458, inference time: 64.05ms, SSIM = 1.0\n","PSRN on ['0096_L.png'] and ['0096_R.png'] : 20.820, inference time: 71.20ms, SSIM = 0.9\n","PSRN on ['0097_L.png'] and ['0097_R.png'] : 19.242, inference time: 60.19ms, SSIM = 0.9\n","PSRN on ['0098_L.png'] and ['0098_R.png'] : 25.076, inference time: 62.29ms, SSIM = 1.0\n","PSRN on ['0099_L.png'] and ['0099_R.png'] : 31.216, inference time: 76.42ms, SSIM = 1.0\n","PSRN on ['0100_L.png'] and ['0100_R.png'] : 22.615, inference time: 58.47ms, SSIM = 1.0\n","PSRN on ['0101_L.png'] and ['0101_R.png'] : 22.297, inference time: 71.05ms, SSIM = 1.0\n","PSRN on ['0102_L.png'] and ['0102_R.png'] : 26.694, inference time: 64.06ms, SSIM = 1.0\n","PSRN on ['0103_L.png'] and ['0103_R.png'] : 31.327, inference time: 58.41ms, SSIM = 1.0\n","PSRN on ['0104_L.png'] and ['0104_R.png'] : 21.438, inference time: 70.95ms, SSIM = 0.9\n","PSRN on ['0105_L.png'] and ['0105_R.png'] : 19.935, inference time: 71.08ms, SSIM = 0.9\n","PSRN on ['0106_L.png'] and ['0106_R.png'] : 25.249, inference time: 71.03ms, SSIM = 1.0\n","PSRN on ['0107_L.png'] and ['0107_R.png'] : 24.773, inference time: 71.04ms, SSIM = 1.0\n","PSRN on ['0108_L.png'] and ['0108_R.png'] : 25.830, inference time: 58.37ms, SSIM = 1.0\n","PSRN on ['0109_L.png'] and ['0109_R.png'] : 36.396, inference time: 71.78ms, SSIM = 1.0\n","PSRN on ['0110_L.png'] and ['0110_R.png'] : 36.609, inference time: 59.82ms, SSIM = 1.0\n","PSRN on ['0111_L.png'] and ['0111_R.png'] : 24.344, inference time: 73.09ms, SSIM = 1.0\n","PSRN on ['0112_L.png'] and ['0112_R.png'] : 33.319, inference time: 73.54ms, SSIM = 1.0\n","Average PSRN: 28.545, average inference time: 212.06ms, average SSIM : 1.0\n","Saving the predicted HR images\n","Testing is done!, predicted HR images are saved in /content/drive/MyDrive/phd/wk1/phase1_baseline/Output_UP_VM_CR_b4_loss_anchor/x2/flkr_500/test_res1\n"]}],"source":["# test\n","\n","import os\n","import argparse\n","import torch\n","import time\n","\n","#from model import XLSR\n","#from dataset import create_dataloader\n","#from metric import cal_psnr\n","#from visualization import save_res\n","\n","\n","def test(model, dataloader, device, txt_path):\n","    pred_list_l = []\n","    pred_list_r = []\n","    name_list_l = []\n","    name_list_r = []\n","    avg_psnr = 0.\n","    avg_time = 0.\n","    avg_ssim = 0.\n","    with torch.no_grad():\n","        # print(\"warm up ...\")\n","        random_input_l = torch.randn(1, 3, 640, 360).to(device)\n","        random_input_r = torch.randn(1, 3, 640, 360).to(device)\n","\n","        print(\"Start the inference ...\")\n","        for LR_img_l, LR_img_r, HR_img_l, HR_img_r, img_nam_l , img_nam_r in dataloader:\n","            LR_img_l, LR_img_r = LR_img_l.to(device).float(), LR_img_r.to(device).float()\n","            HR_img_l, HR_img_r = HR_img_l.to(device).float(), HR_img_r.to(device).float()\n","            if device != 'cpu':\n","                torch.cuda.synchronize()\n","            t0 = time.perf_counter()\n","            # print('LR shape = ', LR_img_l.shape,LR_img_r.shape )\n","            HR_pred_l, HR_pred_r = model(LR_img_l, LR_img_r, is_training=0)\n","            # HR_pred_l, HR_pred_r = HR_pred[:, :3, :, :], HR_pred[:, 3:, :, :]\n","            # print('HR_Predictions shape = ', HR_pred_l.shape,HR_pred_r.shape )\n","            if device != 'cpu':\n","                torch.cuda.synchronize()\n","            t1 = time.perf_counter()\n","            psnr_l = cal_psnr(HR_pred_l, HR_img_l).item()\n","            # psnr_r = cal_psnr(HR_pred_r, HR_img_r).item()\n","            psnr = psnr_l\n","            inference_time = t1 - t0\n","            ssim_l = compute_SSIM(HR_pred_l, HR_img_l).item()\n","            print(f\"PSRN on {img_nam_l} and {img_nam_r} : {psnr:.3f}, inference time: {1000*inference_time:.2f}ms, SSIM = {ssim_l:.1f}\")\n","            with open(txt_path, 'a') as f:\n","                f.write(f\"PSRN on {img_nam_l} and {img_nam_r} : {psnr:.3f}, inference time: {1000*inference_time:.2f}ms, SSIM = {ssim_l:.1f}\" + '\\n')\n","            avg_psnr += psnr\n","            avg_time += inference_time\n","            avg_ssim += ssim_l\n","            pred_list_l.append(HR_pred_l)\n","            pred_list_r.append(HR_pred_r)\n","            name_list_l += img_nam_l\n","            name_list_r += img_nam_r\n","    avg_psnr /= len(test_dataloader)\n","    avg_time /= len(test_dataloader)\n","    avg_ssim /= len(test_dataloader)\n","    print(f\"Average PSRN: {avg_psnr:.3f}, average inference time: {1000*avg_time:.2f}ms, average SSIM : {avg_ssim:.1f}\")\n","    with open(txt_path, 'a') as f:\n","        f.write(f\"Average PSRN: {avg_psnr:.3f}, average inference time: {1000*avg_time:.2f}ms, average SSIM : {avg_ssim:.1f}\")\n","    return pred_list_l,pred_list_r, name_list_l, name_list_r\n","\n","'''\n","if __name__ == '__main__':\n","    parser = argparse.ArgumentParser()\n","    parser.add_argument('--save-dir', type=str, default='exp/OneCyclicLR_exp0', help='hyperparameters path')\n","    parser.add_argument('--SR-rate', type=int, default=3, help='the scale rate for SR')\n","    parser.add_argument('--model', type=str, default='', help='the path to the saved model')\n","    parser.add_argument('--device', type=str, default='cpu', help='gpu id or \"cpu\"')\n","    opt = parser.parse_args()\n","'''\n","save_dir = '/content/drive/MyDrive/phd/wk1/phase1_baseline/Output_UP_VM_CR_b4_loss_anchor/x2/flkr_500'\n","SR_rate = 2\n","# model_path = '/content/drive/MyDrive/phd/wk1_in_smriti_07_23_gmail/try2/output5_2/best.pt'\n","model_path = '/content/drive/MyDrive/phd/wk1/phase1_baseline/Output_UP_VM_CR_b4_loss_anchor/x2/output1/best.pt'\n","device1 = 'cuda'\n","# device1 = 'cpu'\n","img_name = 1\n","opt = [save_dir, SR_rate, model, device1]\n","\n","os.makedirs(save_dir, exist_ok=True)\n","\n","# cuDnn configurations\n","\n","\n","if device1 != 'cpu':\n","        torch.backends.cudnn.benchmark = True\n","        torch.backends.cudnn.deterministic = True\n","\n","# txt file to record training process\n","txt_path = os.path.join(save_dir, f\"test_res_{img_name}.txt\")\n","if os.path.exists(txt_path):\n","        os.remove(txt_path)\n","\n","    # folder to save the predicted HR image in the validation\n","test_folder = os.path.join(save_dir, f\"test_res{img_name}\")\n","os.makedirs(test_folder, exist_ok=True)\n","\n","# device = 'cuda:' + str(device1) if device1 != 'cpu' else 'cpu'\n","device = device1\n","model = XLSR_stereo_AM(SR_rate)\n","\n","# load pretrained model\n","if model_path.endswith('.pt') and os.path.exists(model_path):\n","        # model.load_state_dict(torch.load(model, map_location=device))\n","        model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n","else:\n","        model.load_state_dict(torch.load(os.path.join(save_dir, 'best.pt'), map_location=device))\n","model.to(device)\n","model.eval()\n","\n","test_dataloader = create_dataloader('test', SR_rate, False, batch_size=1, shuffle=False, num_workers=1)\n","print(len(test_dataloader))\n"," # evaluate\n","# pred_HR_l,pred_HR_r, img_names_l,img_names_r, valid_folder = test(model, test_dataloader, device, txt_path)\n","pred_HR_l,pred_HR_r, img_names_l,img_names_r = test(model, test_dataloader, device, txt_path)\n","\n","print(\"Saving the predicted HR images\")\n","save_res(pred_HR_l,pred_HR_r, img_names_l,img_names_r, test_folder)\n","print(f\"Testing is done!, predicted HR images are saved in {test_folder}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3B6MX6VYjrR_"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[{"file_id":"10jBpN5vt5-ejf5ksKGZJq8OaSn3J56cZ","timestamp":1711434455900},{"file_id":"1bqIbB9A-fB_18VdrOEFUix-BCf0tNr4b","timestamp":1710308462141},{"file_id":"1ONiy1uZaB4VR1CMRA8o1DXJpikSTKkd2","timestamp":1709665176718},{"file_id":"1hVpXaKofFX_asx6M3fUNWqDumlEck5pg","timestamp":1709450905543},{"file_id":"15KH3ZIsXHuMaB3gn-57NA9YSAxz-ehB0","timestamp":1709101945556},{"file_id":"1a1GDpKKCz6BXECjRY1bLNj5Egfkz8I17","timestamp":1709006344635},{"file_id":"131KZpkCKEZehTfOTxaZKwNPFv4vc-uo-","timestamp":1691311932071},{"file_id":"1_ybp1OzG1u3v3w2xQyyPsQSEPS8He0L9","timestamp":1689310796413},{"file_id":"1nLij8NtEpH6mieiftPDjXYnQpYrXrvUh","timestamp":1687593635726}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}